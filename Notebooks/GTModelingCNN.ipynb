{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5005b1c-bcfc-4b9c-8808-a075d3c86532",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.onnx\n",
    "import pandas as pd\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "842688a7-4090-4698-9783-a36d52c62cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "DATA_DIR = 'block_dataset'\n",
    "CHUNK_WIDTH = 16\n",
    "CHUNK_HEIGHT = 256\n",
    "CHUNK_DEPTH = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b32e68b1-1e35-4237-8ded-8aefacc6b83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all CSVs\n",
    "csv_files = [os.path.join(DATA_DIR, f) for f in os.listdir(DATA_DIR) if f.endswith('.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9d02c28-f23b-468a-aae3-b1a834e46cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical fields to encode\n",
    "categorical_fields = [\n",
    "    'Block_ID',\n",
    "    'Block_to_Left',\n",
    "    'Block_to_Right',\n",
    "    'Block_Below',\n",
    "    'Block_Above',\n",
    "    'Block_in_Front',\n",
    "    'Block_Behind',\n",
    "    'ChunkBiome',\n",
    "    'Biome',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e48d88c0-3599-453e-863a-63362420f885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect unique categories across all files\n",
    "label_encoders = {field: LabelEncoder() for field in categorical_fields}\n",
    "all_values = {field: [] for field in categorical_fields}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36993a97-6a43-404c-8b7d-f2870005f489",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    for field in categorical_fields:\n",
    "        all_values[field].extend(df[field].dropna().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da711328-c462-4800-8dd7-a47a3aa8743f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit encoders\n",
    "for field in categorical_fields:\n",
    "    label_encoders[field].fit(all_values[field])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd85f1c7-2c0f-4e7c-a74f-6dd5db05af55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Container for chunk tensors\n",
    "chunk_inputs = []\n",
    "chunk_outputs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d19ed0c-dc82-4986-bd87-da83906868ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_keys = [\n",
    "    'ChunkBiome', 'Biome', 'Block_to_Left', 'Block_to_Right',\n",
    "    'Block_Below', 'Block_Above', 'Block_in_Front', 'Block_Behind'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2c55cd5-4fc5-4cbf-b8b0-05cf691faa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    # Filter out-of-bounds rows early\n",
    "    df = df[(df['x'] < CHUNK_WIDTH) & (df['y'] < CHUNK_HEIGHT) & (df['z'] < CHUNK_DEPTH)]\n",
    "    \n",
    "    # Normalize light level (assuming original range is 0 to 15)\n",
    "    df['Light_Level'] = df['Light_Level'] / 15.0\n",
    "\n",
    "    # Encode all label features in bulk\n",
    "    for key in feature_keys + ['Block_ID']:\n",
    "        df[key] = label_encoders[key].transform(df[key])\n",
    "\n",
    "    # Initialize arrays\n",
    "    chunk_input = np.zeros((CHUNK_WIDTH, CHUNK_HEIGHT, CHUNK_DEPTH, 10), dtype=np.float32)\n",
    "    chunk_output = np.full((CHUNK_WIDTH, CHUNK_HEIGHT, CHUNK_DEPTH), -1, dtype=np.int64)\n",
    "\n",
    "    # Use itertuples for speed\n",
    "    for row in df.itertuples(index=False):\n",
    "        x, y, z = int(row.x), int(row.y), int(row.z)\n",
    "\n",
    "        features = [\n",
    "            row.ChunkBiome,\n",
    "            row.Biome,\n",
    "            float(row.Is_Surface),\n",
    "            float(row.Light_Level),\n",
    "            row.Block_to_Left,\n",
    "            row.Block_to_Right,\n",
    "            row.Block_Below,\n",
    "            row.Block_Above,\n",
    "            row.Block_in_Front,\n",
    "            row.Block_Behind,\n",
    "        ]\n",
    "        chunk_input[x, y, z] = features\n",
    "        chunk_output[x, y, z] = row.Block_ID\n",
    "\n",
    "    # Convert and permute\n",
    "    chunk_inputs.append(torch.tensor(chunk_input).permute(3, 0, 1, 2))  # [C, X, Y, Z]\n",
    "    chunk_outputs.append(torch.tensor(chunk_output))  # [X, Y, Z]\n",
    "\n",
    "# Final stacked tensors\n",
    "X = torch.stack(chunk_inputs)  # [N, C, X, Y, Z]\n",
    "y = torch.stack(chunk_outputs)  # [N, X, Y, Z]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a251e96a-d206-4074-8204-c3289699ad92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle input/output pairs\n",
    "X, y = shuffle(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "599e4c33-42b6-4169-84f2-c66b983fa84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Terrain3DCNN(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv3d(in_channels, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv3d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv3d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv3d(128, 64, kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv3d(64, num_classes, kernel_size=1)  # output logits for each class\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.conv5(x)  # [N, num_classes, X, Y, Z]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b01dab0-a98f-4825-8055-39a4565db47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5462018b-ece2-4c6c-a983-301bc8db9e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = X.shape[1]\n",
    "num_classes = len(label_encoders['Block_ID'].classes_)\n",
    "model = Terrain3DCNN(in_channels, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64cba092-ad1e-409b-9a0f-41f5c73759c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [N, C, X, Y, Z] and [N, X, Y, Z]\n",
    "dataset = TensorDataset(X, y)\n",
    "train_loader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c75665a-b107-48d4-b8b1-ff24be47c39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=-1)  # ignore unassigned voxels\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e02d1e-f9aa-4057-afcb-1e2d4c7ed2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1000\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X = batch_X.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(batch_X)  # shape: [B, num_classes, X, Y, Z]\n",
    "\n",
    "        # Reshape for loss: flatten logits and targets\n",
    "        loss = criterion(\n",
    "            logits.view(-1, num_classes),\n",
    "            batch_y.view(-1)\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcac74ae-1280-4731-95ed-c76f3f88f4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Terrain3DCNN(in_channels=10, num_classes=54)  # Use 54 if that was the number of classes during training\n",
    "model.load_state_dict(torch.load('terrain_model.pth', weights_only=True))  # Avoid the FutureWarning\n",
    "model.eval()\n",
    "\n",
    "# Create a dummy input with the same shape your model expects (N, C, X, Y, Z)\n",
    "dummy_input = torch.randn(1, 10, 16, 256, 16)  # Example: batch size 1, 10 input channels\n",
    "\n",
    "# Export the model to ONNX format\n",
    "torch.onnx.export(model, dummy_input, \"terrain_model.onnx\", verbose=True, input_names=['input'], output_names=['output'])\n",
    "\n",
    "print(\"Model has been successfully exported to ONNX format!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f599b9d-e9ba-47c0-a978-2ef427f07489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights + biases\n",
    "model = Terrain3DCNN(in_channels=10, num_classes=54)\n",
    "\n",
    "# Load the model weights\n",
    "model.load_state_dict(torch.load('terrain_model.pth', weights_only=True))\n",
    "model.eval()\n",
    "\n",
    "# Iterate through the model parameters (weights and biases)\n",
    "for name, param in model.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        print(f\"Weight - {name}: {param.shape}\")\n",
    "        print(param.data)  # Prints the weight values\n",
    "    elif 'bias' in name:\n",
    "        print(f\"Bias - {name}: {param.shape}\")\n",
    "        print(param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9846071-6967-4ef7-b2fa-ed053ff88044",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoders = {field: LabelEncoder() for field in categorical_fields}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35d9fa6-56cd-4283-b1e7-91fcbffb4b2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "label_encoders = {field: LabelEncoder() for field in categorical_fields}\n",
    "all_values = {field: [] for field in categorical_fields}\n",
    "\n",
    "# Collect values from each CSV\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    for field in categorical_fields:\n",
    "        all_values[field].extend(df[field].dropna().tolist())\n",
    "\n",
    "# Fit the encoders\n",
    "for field in categorical_fields:\n",
    "    label_encoders[field].fit(all_values[field])\n",
    "\n",
    "# Now check the encoded categories\n",
    "for field, encoder in label_encoders.items():\n",
    "    if hasattr(encoder, 'classes_'):\n",
    "        print(f\"\\n{field} Encoding:\")\n",
    "        for i, label in enumerate(encoder.classes_):\n",
    "            print(f'\"{label}\": {i}')\n",
    "    else:\n",
    "        print(f\"{field} encoder has not been fitted yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6a83ae75-ad2c-4c38-9619-7e67170bcd05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved block ID mapping to block_id_mapping.json\n"
     ]
    }
   ],
   "source": [
    "idx_to_block = {i: name for i, name in enumerate(label_encoders['Block_ID'].classes_)}\n",
    "with open(\"block_id_mapping.json\", \"w\") as f:\n",
    "    json.dump(idx_to_block, f)\n",
    "\n",
    "print(\"Saved block ID mapping to block_id_mapping.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
